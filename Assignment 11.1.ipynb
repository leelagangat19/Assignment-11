{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1-What are the three stages to build the hypotheses or model in machine learning?\n",
    "\n",
    "Ans:- Fallowing are the three stages to build the model in machine learning :\n",
    "\n",
    "    1- Data Preprocessing :- when we get a set of data to build the machine learning model, it requires different pre-processing action to clean the data like formatting, cleaning and sampling.\n",
    "\n",
    "    2- Build and Testing the model :- when our data gets cleaned up we apply a fraction of that data on an suitable machine learing algorithm. Algorithm will train itself by that data and then builds a model, after model formation we apply unseen data to the model to test the model.\n",
    "\n",
    "    3- Applying the model :- After building and testing the model we apply that model in the real world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2 What is the standard approach to supervised learning?¶\n",
    "\n",
    "Ans: In supervised learning we get labelled data for building the model.\n",
    "\n",
    "So first we split the data in training and testing dataset and then build model on training data and apply testing data on model to test.\n",
    "we'll test the model by comparing the predicted value by the model to the labelled values of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3-What is Training set and Test set?¶\n",
    "\n",
    "Ans: when we get a data set we divide them into Training and Test set.\n",
    "\n",
    "    \n",
    "Trainint Set: In Machine Learning, a training set is a dataset used to train a model. In training the model, specific features are picked out from the training set.These features are then incorporated into the model.Thereby, if the training set is labeled correctly, the model should be able to learn something from these features.\n",
    "\n",
    "Test set :The test set is a dataset used to measure how well the model performs at making predictions on that test set.\n",
    "If the prediction scores for the test set are unreasonable, we’ll have to make some adjustments to our model\n",
    "and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4-What is the general principle of an ensemble method and what is bagging and\n",
    "boosting in ensemble method?\n",
    "\n",
    "Ans :\n",
    "Ensemble Learning method :- It means grouping multiple week learning models to form a strong model so that we can obtain better prediction.\n",
    "\n",
    "Begging :- It is uses in Low Bias - High Variance problem. this problem occur when the model overfits like in Decision tree.\n",
    "\n",
    "In this case various models are built in parallel and each model gets trained on randomly selected samples.\n",
    "then the various models vote to give the final prediction.\n",
    "predictions will be averaged to get the final prediction(in case of regression) and in case of classification the final prediction will be the mode of the predicted ans.\n",
    "Boosting :- It is uses in Low variance - High Bias problem. this problem occures when model underfits.\n",
    "\n",
    "In this method we first sampled the input data to generate a set of training data.\n",
    "then we run an algorithm on this training data to get a trained model.\n",
    "then we take all our training data to test the model and we are gonna discover that some of the points are not well predicted.\n",
    "now we have to build the second bag of sampled data. In this also the data will be randomly choosen, but now each data point is weighted according to error found in last model. So these values are more likely to get picked in this bag than any other data.\n",
    "now we'll build a model for this sample set also then we'll test it.\n",
    "Here the testing will be performed on both of the model and the result will be mode of the both result( in case of classification) in case of regression result will be mean of the both result.\n",
    "now again we'll find some values which are not predicted well. so we'll build one more bag and one more model and this process will continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5-How can you avoid overfitting?\n",
    "\n",
    "Ans : when we train the model on training set to such a level that it starts predicting noise or outlier present in training data correctly, then we say that model gets overfit. because in this case model will show 100% accuracy for training set but its accuracy will be low on test data.\n",
    "there are multiple methods by which we can avoid overfitting -\n",
    "\n",
    "1- Ensamble method :- it is the best method to avoid overfitting and increasing accuracy.\n",
    "\n",
    "2- Remove features :- its example is Pruning, which we do in case of decision tree model.\n",
    "\n",
    "3- Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
